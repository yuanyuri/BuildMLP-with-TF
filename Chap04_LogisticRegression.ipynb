{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              age        chd\n",
      "count  100.000000  100.00000\n",
      "mean    44.380000    0.43000\n",
      "std     11.721327    0.49757\n",
      "min     20.000000    0.00000\n",
      "25%     34.750000    0.00000\n",
      "50%     44.000000    0.00000\n",
      "75%     55.000000    1.00000\n",
      "max     69.000000    1.00000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"CHD.csv\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(df)\n",
    "learning_rate = 0.05\n",
    "batch_size = 20\n",
    "total_batches = int(total_size / batch_size)\n",
    "display_step = 1\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "onc = OneHotEncoder(sparse=False)\n",
    "data = ss.fit_transform(df[\"age\"].values.reshape(-1, 1)).astype(\"float32\")\n",
    "label = onc.fit_transform(df[\"chd\"].values.reshape(-1, 1)).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_id: 0, W: -0.3948, b: 0.2895, loss: 13.1591\n",
      "epoch_id: 1, W: -0.6532, b: 0.1740, loss: 10.9881\n",
      "epoch_id: 2, W: -0.7638, b: 0.1797, loss: 10.6938\n",
      "epoch_id: 3, W: -0.8254, b: 0.1754, loss: 10.9186\n",
      "epoch_id: 4, W: -0.7815, b: 0.1883, loss: 10.9220\n",
      "epoch_id: 5, W: -0.7140, b: 0.1983, loss: 10.8467\n",
      "epoch_id: 6, W: -0.6469, b: 0.2186, loss: 10.7929\n",
      "epoch_id: 7, W: -0.6278, b: 0.1913, loss: 10.7776\n",
      "epoch_id: 8, W: -0.6661, b: 0.2140, loss: 10.8441\n",
      "epoch_id: 9, W: -0.6326, b: 0.2122, loss: 10.7680\n",
      "epoch_id: 10, W: -0.6327, b: 0.1764, loss: 10.7501\n",
      "epoch_id: 11, W: -0.6389, b: 0.1471, loss: 10.7778\n",
      "epoch_id: 12, W: -0.6369, b: 0.2057, loss: 10.8023\n",
      "epoch_id: 13, W: -0.6422, b: 0.2054, loss: 10.7564\n",
      "epoch_id: 14, W: -0.6176, b: 0.1580, loss: 10.8095\n",
      "epoch_id: 15, W: -0.6508, b: 0.1884, loss: 10.7689\n",
      "epoch_id: 16, W: -0.6272, b: 0.2115, loss: 10.7718\n",
      "epoch_id: 17, W: -0.6138, b: 0.2206, loss: 10.8213\n",
      "epoch_id: 18, W: -0.6570, b: 0.1696, loss: 10.7558\n",
      "epoch_id: 19, W: -0.6846, b: 0.1710, loss: 10.7597\n",
      "epoch_id: 20, W: -0.6506, b: 0.1881, loss: 10.8245\n",
      "epoch_id: 21, W: -0.6373, b: 0.1927, loss: 10.7563\n",
      "epoch_id: 22, W: -0.6763, b: 0.2059, loss: 10.8151\n",
      "epoch_id: 23, W: -0.6478, b: 0.2160, loss: 10.8005\n",
      "epoch_id: 24, W: -0.6437, b: 0.1704, loss: 10.7600\n",
      "epoch_id: 25, W: -0.6577, b: 0.1209, loss: 10.8988\n",
      "epoch_id: 26, W: -0.6890, b: 0.1734, loss: 10.8256\n",
      "epoch_id: 27, W: -0.6599, b: 0.2432, loss: 10.7590\n",
      "epoch_id: 28, W: -0.6065, b: 0.2345, loss: 10.8342\n",
      "epoch_id: 29, W: -0.6511, b: 0.1897, loss: 10.8534\n",
      "epoch_id: 30, W: -0.6093, b: 0.2098, loss: 10.8522\n",
      "epoch_id: 31, W: -0.6061, b: 0.1962, loss: 10.7697\n",
      "epoch_id: 32, W: -0.6490, b: 0.1745, loss: 10.7510\n",
      "epoch_id: 33, W: -0.6618, b: 0.1853, loss: 10.7467\n",
      "epoch_id: 34, W: -0.6808, b: 0.1967, loss: 10.7824\n",
      "epoch_id: 35, W: -0.7413, b: 0.1614, loss: 10.9543\n",
      "epoch_id: 36, W: -0.6638, b: 0.1957, loss: 10.7946\n",
      "epoch_id: 37, W: -0.6342, b: 0.2316, loss: 10.7622\n",
      "epoch_id: 38, W: -0.6143, b: 0.1963, loss: 10.7930\n",
      "epoch_id: 39, W: -0.6045, b: 0.2146, loss: 10.7869\n",
      "epoch_id: 40, W: -0.6675, b: 0.1866, loss: 10.8110\n",
      "epoch_id: 41, W: -0.6622, b: 0.1515, loss: 10.7801\n",
      "epoch_id: 42, W: -0.6313, b: 0.1824, loss: 10.7989\n",
      "epoch_id: 43, W: -0.6154, b: 0.2026, loss: 10.7843\n",
      "epoch_id: 44, W: -0.6440, b: 0.1890, loss: 10.7857\n",
      "epoch_id: 45, W: -0.6732, b: 0.1966, loss: 10.7689\n",
      "epoch_id: 46, W: -0.6917, b: 0.1794, loss: 10.7935\n",
      "epoch_id: 47, W: -0.6613, b: 0.1879, loss: 10.7624\n",
      "epoch_id: 48, W: -0.5891, b: 0.2072, loss: 10.8466\n",
      "epoch_id: 49, W: -0.6226, b: 0.2518, loss: 10.8142\n",
      "epoch_id: 50, W: -0.6831, b: 0.2251, loss: 10.7952\n",
      "epoch_id: 51, W: -0.6770, b: 0.1624, loss: 10.7876\n",
      "epoch_id: 52, W: -0.6743, b: 0.1265, loss: 10.8246\n",
      "epoch_id: 53, W: -0.6418, b: 0.1717, loss: 10.7502\n",
      "epoch_id: 54, W: -0.6166, b: 0.2474, loss: 10.7869\n",
      "epoch_id: 55, W: -0.6672, b: 0.2288, loss: 10.8223\n",
      "epoch_id: 56, W: -0.6650, b: 0.1795, loss: 10.7844\n",
      "epoch_id: 57, W: -0.6383, b: 0.2209, loss: 10.8116\n",
      "epoch_id: 58, W: -0.5951, b: 0.2071, loss: 10.8219\n",
      "epoch_id: 59, W: -0.5949, b: 0.2088, loss: 10.8931\n",
      "epoch_id: 60, W: -0.6607, b: 0.0980, loss: 10.8793\n",
      "epoch_id: 61, W: -0.6720, b: 0.1332, loss: 10.8293\n",
      "epoch_id: 62, W: -0.6858, b: 0.2310, loss: 10.8180\n",
      "epoch_id: 63, W: -0.6788, b: 0.2538, loss: 10.7961\n",
      "epoch_id: 64, W: -0.6504, b: 0.2462, loss: 10.8128\n",
      "epoch_id: 65, W: -0.6775, b: 0.1358, loss: 10.9077\n",
      "epoch_id: 66, W: -0.6364, b: 0.1611, loss: 10.8040\n",
      "epoch_id: 67, W: -0.6338, b: 0.1937, loss: 10.7756\n",
      "epoch_id: 68, W: -0.5973, b: 0.1746, loss: 10.8210\n",
      "epoch_id: 69, W: -0.6021, b: 0.1758, loss: 10.8320\n",
      "epoch_id: 70, W: -0.6443, b: 0.2435, loss: 10.8698\n",
      "epoch_id: 71, W: -0.7274, b: 0.2140, loss: 10.8092\n",
      "epoch_id: 72, W: -0.7038, b: 0.1473, loss: 10.8856\n",
      "epoch_id: 73, W: -0.6706, b: 0.1826, loss: 10.7595\n",
      "epoch_id: 74, W: -0.6104, b: 0.2218, loss: 10.7893\n",
      "epoch_id: 75, W: -0.6251, b: 0.2521, loss: 10.8041\n",
      "epoch_id: 76, W: -0.6592, b: 0.1980, loss: 10.8019\n",
      "epoch_id: 77, W: -0.6450, b: 0.1767, loss: 10.7630\n",
      "epoch_id: 78, W: -0.6566, b: 0.1792, loss: 10.7630\n",
      "epoch_id: 79, W: -0.6138, b: 0.2063, loss: 10.8393\n",
      "epoch_id: 80, W: -0.6239, b: 0.1435, loss: 10.8387\n",
      "epoch_id: 81, W: -0.6402, b: 0.1799, loss: 10.7638\n",
      "epoch_id: 82, W: -0.6707, b: 0.2026, loss: 10.7642\n",
      "epoch_id: 83, W: -0.6408, b: 0.2504, loss: 10.8573\n",
      "epoch_id: 84, W: -0.6797, b: 0.1892, loss: 10.8051\n",
      "epoch_id: 85, W: -0.6501, b: 0.2002, loss: 10.7749\n",
      "epoch_id: 86, W: -0.6527, b: 0.1683, loss: 10.7730\n",
      "epoch_id: 87, W: -0.6643, b: 0.1599, loss: 10.7730\n",
      "epoch_id: 88, W: -0.6218, b: 0.2097, loss: 10.7930\n",
      "epoch_id: 89, W: -0.6468, b: 0.2279, loss: 10.7731\n",
      "epoch_id: 90, W: -0.6667, b: 0.2322, loss: 10.7884\n",
      "epoch_id: 91, W: -0.6573, b: 0.1722, loss: 10.7546\n",
      "epoch_id: 92, W: -0.6365, b: 0.1616, loss: 10.7594\n",
      "epoch_id: 93, W: -0.6173, b: 0.1784, loss: 10.7665\n",
      "epoch_id: 94, W: -0.6482, b: 0.2133, loss: 10.7913\n",
      "epoch_id: 95, W: -0.6654, b: 0.1964, loss: 10.7636\n",
      "epoch_id: 96, W: -0.6629, b: 0.1923, loss: 10.7561\n",
      "epoch_id: 97, W: -0.6394, b: 0.1695, loss: 10.7676\n",
      "epoch_id: 98, W: -0.6581, b: 0.1866, loss: 10.7649\n",
      "epoch_id: 99, W: -0.6093, b: 0.2204, loss: 10.8100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def model(X, W, b):\n",
    "    Z = tf.matmul(X, W) + b\n",
    "    return tf.nn.softmax(Z)\n",
    "W = tf.get_variable(name=\"W\", shape=[1,2], dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "b = tf.get_variable(name=\"b\", shape=[1,2], dtype=tf.float32, initializer=tf.zeros_initializer)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-1)\n",
    "variables = [W, b]\n",
    "for epoch_id in range(epochs):\n",
    "    data, label = shuffle(data, label)\n",
    "    offset = 0\n",
    "    cost = 0.\n",
    "    for i in range(total_batches):\n",
    "        old_offset = offset\n",
    "        offset += batch_size\n",
    "        batch_xs = data[old_offset : offset, :]\n",
    "        batch_ys = label[old_offset : offset, :]\n",
    "        X = tf.constant(batch_xs, dtype=tf.float32)\n",
    "        y = tf.constant(batch_ys, dtype=tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_y = model(X, W, b)\n",
    "            loss = tf.reduce_sum(-tf.reduce_sum(y * tf.log(pred_y), axis=1))\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\n",
    "        cost += loss\n",
    "    cost = cost / total_batches\n",
    "    W_ = W.numpy()[0][0]\n",
    "    b_ = b.numpy()[0][0]\n",
    "    print(\"epoch_id: %d, W: %.4f, b: %.4f, loss: %.4f\" % (epoch_id, W_ , b_, cost))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
